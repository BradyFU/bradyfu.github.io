<head>
<link rel="shortcut icon" href="images/z.ico" type="image/x-icon" />
<meta name="keywords" content="Chaoyou Fu" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />

<title>Chaoyou Fu-Homepage</title>
</head>
<body>
<div id="layout-content">

<table class="imgtable"><tr><td>
<img src="images/v.jpg" alt="alt text" width="300px" height="200px"/> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
    <h1> Chaoyou Fu &nbsp; <img src="./images/name.png" alt="alt text" height="31"> <br /> </h1> 
</div>

<p>
Assistant Professor, Nanjing University
<br />
<a href="https://is.nju.edu.cn/main.htm"> School of Intelligence Science and Technology </a> <br />
<br />
<a href="https://scholar.google.com.hk/citations?user=4A1xYQwAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;|&nbsp; <a href="https://github.com/BradyFU">GitHub</a><br />
Email: <img src="./images/email.png" alt="alt text" height="17"> <br />
</p>
</td></tr></table>
<hr />




<h2>Biography</h2>
<p>
I am an Assistant Professor at Nanjing University, under the leadership of <a href="https://caifeng-shan.github.io/">Prof. Caifeng Shan</a>, <a href="http://people.ucas.ac.cn/~heran">Prof. Ran He</a>, and <a href="https://is.nju.edu.cn/ttn/main.htm">Prof. Tieniu Tan</a>. Before that, I was a Senior Researcher at <a href="https://open.youtu.qq.com/#/open">Tencent Youtu Lab</a>, engaged in academic research and engineering landing works as a Technology & Project Leader, from 2022 to 2024. I obtained my Ph.D. degree from <a href="http://www.cripac.ia.ac.cn/CN/model/index.htm">NLPR-CASIA</a> in 2022, under the supervision of <a href="http://people.ucas.ac.cn/~heran">Prof. Ran He</a>.
</p>
<br />
<p>
My current research interests mainly focus on Multimodal LLM, LLM, and biometrics.
</p>
<br />
<p>
<span style="color: rgb(255, 0, 0)"> <strong> We are looking for self-motivated PhD and Master candidates! If you are interested, please feel free to contact me. Meanwhile, I am open to any discussion or collaboration.  </strong> </span> 
</p>




<h2>Selected Publications</h2>
<table class="imgtable"><tr><td>
<img src="images/paper_vita.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2408.05211">VITA: Towards Open-Source Interactive Omni Multimodal LLM</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Haojia Lin, Zuwei Long, Yunhang Shen, et al <br />
arXiv 2024, <a href="https://vita-home.github.io/">Project</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_videomme.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2405.21075">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, et al <br />
arXiv 2024, <a href="https://video-mme.github.io/">Project</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_mme.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2306.13394.pdf">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, et al <br />
arXiv 2023, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">Leaderboard [with 50+ MLLMs ğŸŒŸ]</a>, <a href="images/bib_mme.txt"><strong>Citation</strong></a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_mllm_v2.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2306.13549.pdf">A Survey on Multimodal Large Language Models</a> <br />
Shukang Yin, <strong>Chaoyou Fu [Project Leader]</strong>, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen <br />
arXiv 2023 (update on April 2, 2024), <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Project [10k+ Stars ğŸŒŸ]</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_woodpecker.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2310.16045.pdf">Woodpecker: Hallucination Correction for Multimodal Large Language Models</a> <br />
Shukang Yin, <strong>Chaoyou Fu [Project Leader]</strong>, Sirui Zhao, Tong Xu, et al <br />
arXiv 2023, <a href="https://github.com/BradyFU/Woodpecker">Code [500+ Stars ğŸŒŸ]</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_gemini_gpt4v.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2312.12436.pdf">A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang, et al <br />
arXiv 2023, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Project</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_ape.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2312.02153.pdf">APE: Aligning and Prompting Everything All at Once for Universal Visual Perception</a> <br />
Yunhang Shen, <strong>Chaoyou Fu</strong>, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, et al <br />
CVPR 2024, <a href="https://github.com/shenyunhang/APE">Code [400+ Stars ğŸŒŸ]</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_dvg-face.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2009.09399.pdf">DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition</a> <br />
<strong>Chaoyou Fu</strong>, Xiang Wu, Yibo Hu, Huaibo Huang, and Ran He <br />
TPAMI 2022, <a href="https://github.com/BradyFU/DVG-Face">Code</a> 
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_safe.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://ieeexplore.ieee.org/abstract/document/9971748">Towards Lightweight Pixel-Wise Hallucination for Heterogeneous Face Recognition</a> <br />
<strong>Chaoyou Fu</strong>, Xiaoqiang Zhou, Weizan He, and Ran He <br />
TPAMI 2023
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_mvf.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/1903.12003.pdf">High Fidelity Face Manipulation with Extreme Poses and Expressions</a> <br />
<strong>Chaoyou Fu</strong>, Yibo Hu, Xiang Wu, Guoli Wang, Qian Zhang, and Ran He <br />
TIFS 2021, <a href="./projects/mvf-hq/mvf-hq.html">Dataset</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_cm-nas.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_CM-NAS_Cross-Modality_Neural_Architecture_Search_for_Visible-Infrared_Person_Re-Identification_ICCV_2021_paper.pdf">CM-NAS: Cross-Modality Neural Architecture Search for Visible-Infrared Person Re-Identification</a> <br />
<strong>Chaoyou Fu</strong>, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei, and Ran He <br />
ICCV 2021, <a href="https://github.com/JDAI-CV/CM-NAS">Code</a>
</p>
</td></tr></table>




<h2>Academic Services</h2> 
<ul>
<li><p> Conference Reviewer: NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV, AAAI, ACM MM, IJCAI </p></li>
<li><p> Journal Reviewer: IEEE TIP, PR </p></li>
</ul>




<h2>Honors and Awards</h2> 
<ul>
<li><p> [2023.12] åŒ—äº¬å¸‚ä¼˜ç§€åšå£«å­¦ä½è®ºæ–‡ </p></li>
<li><p> [2023.08] ä¸­å›½ç§‘å­¦é™¢ä¼˜ç§€åšå£«å­¦ä½è®ºæ–‡ </p></li>
<li><p> [2023.07] <span style="color: #4f69c6"><strong>IEEE Biometrics Council Best Doctoral Dissertation Award</strong></span> </p></li>
<li><p> [2023.07] CVPR 2023 Outstanding Reviewer (232/7000+) </p></li>
<li><p> [2022.07] <span style="color: #4f69c6"><strong>ä¸­å›½ç§‘å­¦é™¢é™¢é•¿ç‰¹åˆ«å¥–</strong></span> </p></li>
<li><p> [2022.07] åŒ—äº¬å¸‚ä¼˜ç§€æ¯•ä¸šç”Ÿ </p></li>
<li><p> [2021.12] 2022å¹´â€œ<span style="color: #4f69c6"><strong>è…¾è®¯æŠ€æœ¯å¤§å’–</strong></span>â€è®¡åˆ’-T10 </p></li>
<li><p> [2021.12] 2022å¹´â€œ<span style="color: #4f69c6"><strong>é˜¿é‡Œæ˜Ÿ</strong></span>â€è®¡åˆ’-P7 </p></li>
<li><p> [2021.12] åšå£«ç ”ç©¶ç”Ÿå›½å®¶å¥–å­¦é‡‘ </p></li>
<li><p> [2021.11] å®é’¢å¥–å­¦é‡‘ä¼˜ç§€å­¦ç”Ÿå¥– </p></li>
<li><p> [2019.12] ç¡•å£«ç ”ç©¶ç”Ÿå›½å®¶å¥–å­¦é‡‘ </p></li>
<li><p> [2017.06] å®‰å¾½çœä¼˜ç§€æ¯•ä¸šç”Ÿ </p></li>
<li><p> [2015.11] æœ¬ç§‘ç”Ÿå›½å®¶å¥–å­¦é‡‘ </p></li>
<li><p> [2015.08] â€œé£æ€å¡å°”â€æ¯å…¨å›½å¤§å­¦ç”Ÿæ™ºèƒ½æ±½è½¦ç«èµ›å…¨å›½æ€»å†³èµ›äºŒç­‰å¥– </p></li>
</ul>


</body>
</html>
