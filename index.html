<head>
<link rel="shortcut icon" href="images/z.ico" type="image/x-icon" />
<meta name="keywords" content="Chaoyou Fu" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />

<title>Chaoyou Fu-Homepage</title>
</head>
<body>
<div id="layout-content">

<table class="imgtable"><tr><td>
<img src="images/v.jpg" alt="alt text" width="300px" height="200px"/> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
    <h1> Chaoyou Fu &nbsp; <img src="./images/name.png" alt="alt text" height="31"> <br /> </h1> 
</div>

<p>
Researcher & Assistant Professor & PhD Supervisor
<br />
Nanjing University 
<br />
<br />
<a href="https://scholar.google.com.hk/citations?user=4A1xYQwAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;|&nbsp; <a href="https://github.com/BradyFU">GitHub</a><br />  
Email: <img src="./images/email.png" alt="alt text" height="17"> <br />
</p>
</td></tr></table>
<hr />


<h2>Biography</h2>
<p>
I am now working at Nanjing University, with <a href="https://is.nju.edu.cn/ttn/main.htm">Prof. Tieniu Tan</a> and <a href="https://caifeng-shan.github.io/">Prof. Caifeng Shan</a>, and leading <strong>NJU-MIG</strong> (Multimodal Intelligence Group, <strong>Âçó‰∫¨Â§ßÂ≠¶Á±≥Ê†ºÂ∞èÁªÑ</strong>). Before that, I was a Senior Researcher at <a href="https://open.youtu.qq.com/#/open">Tencent Youtu Lab</a>, engaged in academic research and engineering landing works as a Technology & Project Leader, from 2022 to 2024. I obtained my Ph.D. degree from <a href="http://www.cripac.ia.ac.cn/CN/model/index.htm">NLPR-CASIA</a> in 2022, under the supervision of <a href="http://people.ucas.ac.cn/~heran">Prof. Ran He</a>.
</p>
<br />
<p>
My current research interests mainly focus on Multimodal LLM, LLM, and biometrics.
</p>
<br />
<p>
<span style="color: rgb(255, 0, 0)"> <strong> We are looking for self-motivated PhD and Master candidates! If you are interested, please feel free to contact me. Meanwhile, I am open to any discussion or collaboration.  </strong> </span> 
</p>
<br />
<p>
<img src="images/MIG.jpg" alt="alt text" style="width: 80%; height: auto; display: block; margin: auto;"/>
</p>


<h2>Selected Publications</h2>
<table class="imgtable"><tr><td>
<img src="images/paper_mm-rlhf.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2502.10391">MM-RLHF: The Next Step Forward in Multimodal LLM Alignment</a> <br />
Yi-Fan Zhang, Tao Yu, Haochen Tian, <strong>Chaoyou Fu [Corresponding Author]</strong>, Peiyan Li, et al <br />
arXiv 2025, <a href="https://github.com/Kwai-YuanQi/MM-RLHF">Code</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_longvita.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2502.05177">Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuracy</a> <br />
Yunhang Shen, <strong>Chaoyou Fu [Corresponding Author]</strong>, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, et al <br />
arXiv 2025, <a href="https://github.com/VITA-MLLM/Long-VITA">Code</a>
</p>
</td></tr></table>
    
<table class="imgtable"><tr><td>
<img src="images/paper_vita-1.5.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2501.01957">VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Haojia Lin, Xiong Wang, Yi-Fan Zhang, et al <br />
arXiv 2025, <a href="https://github.com/VITA-MLLM/VITA">Code [2k+ Stars üåü]</a>
</p>
</td></tr></table>
    
<table class="imgtable"><tr><td>
<img src="images/paper_mme-survey.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2411.15296">MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Yi-Fan Zhang, Shukang Yin, Bo Li, et al <br />
arXiv 2024, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks">Project</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_vita.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2408.05211">VITA: Towards Open-Source Interactive Omni Multimodal LLM</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Haojia Lin, Zuwei Long, Yunhang Shen, et al <br />
arXiv 2024, <a href="https://vita-home.github.io/">Project</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_videomme.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2405.21075">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, et al <br />
CVPR 2025, <a href="https://video-mme.github.io/">Project</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_mme.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2306.13394.pdf">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, et al <br />
arXiv 2023, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">Leaderboard [with 50+ MLLMs üåü]</a>, <a href="images/bib_mme.txt"><strong>Citation</strong></a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_mllm_v2.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2306.13549.pdf">A Survey on Multimodal Large Language Models</a> <br />
Shukang Yin, <strong>Chaoyou Fu [Project Leader]</strong>, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen <br />
National Science Review 2024, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Project [10k+ Stars üåü]</a>, <a href="images/bib_survey.txt"><strong>Citation</strong></a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_woodpecker.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2310.16045.pdf">Woodpecker: Hallucination Correction for Multimodal Large Language Models</a> <br />
Shukang Yin, <strong>Chaoyou Fu [Project Leader]</strong>, Sirui Zhao, Tong Xu, et al <br />
arXiv 2023, <a href="https://github.com/BradyFU/Woodpecker">Code [500+ Stars üåü]</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_gemini_gpt4v.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2312.12436.pdf">A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang, et al <br />
arXiv 2023, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Project</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_ape.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2312.02153.pdf">APE: Aligning and Prompting Everything All at Once for Universal Visual Perception</a> <br />
Yunhang Shen, <strong>Chaoyou Fu</strong>, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, et al <br />
CVPR 2024, <a href="https://github.com/shenyunhang/APE">Code [500+ Stars üåü]</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_dvg-face.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2009.09399.pdf">DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition</a> <br />
<strong>Chaoyou Fu</strong>, Xiang Wu, Yibo Hu, Huaibo Huang, and Ran He <br />
TPAMI 2022, <a href="https://github.com/BradyFU/DVG-Face">Code</a> 
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_safe.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://ieeexplore.ieee.org/abstract/document/9971748">Towards Lightweight Pixel-Wise Hallucination for Heterogeneous Face Recognition</a> <br />
<strong>Chaoyou Fu</strong>, Xiaoqiang Zhou, Weizan He, and Ran He <br />
TPAMI 2023
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_mvf.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/1903.12003.pdf">High Fidelity Face Manipulation with Extreme Poses and Expressions</a> <br />
<strong>Chaoyou Fu</strong>, Yibo Hu, Xiang Wu, Guoli Wang, Qian Zhang, and Ran He <br />
TIFS 2021, <a href="./projects/mvf-hq/mvf-hq.html">Dataset</a>
</p>
</td></tr></table>


<table class="imgtable"><tr><td>
<img src="images/paper_cm-nas.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_CM-NAS_Cross-Modality_Neural_Architecture_Search_for_Visible-Infrared_Person_Re-Identification_ICCV_2021_paper.pdf">CM-NAS: Cross-Modality Neural Architecture Search for Visible-Infrared Person Re-Identification</a> <br />
<strong>Chaoyou Fu</strong>, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei, and Ran He <br />
ICCV 2021, <a href="https://github.com/JDAI-CV/CM-NAS">Code</a>
</p>
</td></tr></table>




<h2>Academic Services</h2> 
<ul>
<li><p> Conference Reviewer: NeurIPS, ICLR, ICML, CVPR, ICCV, ECCV, AAAI, ACM MM, IJCAI </p></li>
<li><p> Journal Reviewer: IEEE TIP, PR </p></li>
</ul>




<h2>Honors and Awards</h2> 
<ul>
<li><p> [2025.03] <span style="color: #4f69c6"><strong>Á¨¨ÂçÅÂ±ä‰∏≠ÂõΩÁßëÂçèÈùíÂπ¥‰∫∫ÊâçÊâò‰∏æÂ∑•Á®ã</strong></span> </p></li>
<li><p> [2024.11] Â∞èÁ±≥ÈùíÂπ¥Â≠¶ËÄÖ-ÁßëÊäÄÂàõÊñ∞Â•ñ </p></li>
<li><p> [2023.12] Âåó‰∫¨Â∏Ç‰ºòÁßÄÂçöÂ£´Â≠¶‰ΩçËÆ∫Êñá </p></li>
<li><p> [2023.08] ‰∏≠ÂõΩÁßëÂ≠¶Èô¢‰ºòÁßÄÂçöÂ£´Â≠¶‰ΩçËÆ∫Êñá </p></li>
<li><p> [2023.07] <span style="color: #4f69c6"><strong>IEEE Biometrics Council Best Doctoral Dissertation Award</strong></span> </p></li>
<li><p> [2023.07] CVPR 2023 Outstanding Reviewer (232/7000+) </p></li>
<li><p> [2022.07] <span style="color: #4f69c6"><strong>‰∏≠ÂõΩÁßëÂ≠¶Èô¢Èô¢ÈïøÁâπÂà´Â•ñ</strong></span> </p></li>
<li><p> [2022.07] Âåó‰∫¨Â∏Ç‰ºòÁßÄÊØï‰∏öÁîü </p></li>
<li><p> [2021.12] 2022Âπ¥‚Äú<span style="color: #4f69c6"><strong>ËÖæËÆØÊäÄÊúØÂ§ßÂíñ</strong></span>‚ÄùËÆ°Âàí-T10 </p></li>
<li><p> [2021.12] 2022Âπ¥‚Äú<span style="color: #4f69c6"><strong>ÈòøÈáåÊòü</strong></span>‚ÄùËÆ°Âàí-P7 </p></li>
<li><p> [2021.12] ÂçöÂ£´Á†îÁ©∂ÁîüÂõΩÂÆ∂Â•ñÂ≠¶Èáë </p></li>
<li><p> [2021.11] ÂÆùÈí¢Â•ñÂ≠¶Èáë‰ºòÁßÄÂ≠¶ÁîüÂ•ñ </p></li>
<li><p> [2019.12] Á°ïÂ£´Á†îÁ©∂ÁîüÂõΩÂÆ∂Â•ñÂ≠¶Èáë </p></li>
<li><p> [2017.06] ÂÆâÂæΩÁúÅ‰ºòÁßÄÊØï‰∏öÁîü </p></li>
<li><p> [2015.11] Êú¨ÁßëÁîüÂõΩÂÆ∂Â•ñÂ≠¶Èáë </p></li>
<li><p> [2015.08] ‚ÄúÈ£ûÊÄùÂç°Â∞î‚ÄùÊùØÂÖ®ÂõΩÂ§ßÂ≠¶ÁîüÊô∫ËÉΩÊ±ΩËΩ¶Á´ûËµõÂÖ®ÂõΩÊÄªÂÜ≥Ëµõ‰∫åÁ≠âÂ•ñ </p></li>
</ul>


</body>
</html>
