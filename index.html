<head>
<link rel="shortcut icon" href="images/z.ico" type="image/x-icon" />
<meta name="keywords" content="Chaoyou Fu" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />

<title>Chaoyou Fu-Homepage</title>
</head>
<body>
<div id="layout-content">

<table class="imgtable"><tr><td>
<img src="images/v.jpg" alt="alt text" width="300px" height="200px"/> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
    <h1> Chaoyou Fu &nbsp; <img src="./images/name.png" alt="alt text" height="31"> <br /> </h1> 
</div>

<p>
Researcher & Assistant Professor & PhD Supervisor
<br />
Nanjing University 
<br />
<br />
<a href="https://scholar.google.com.hk/citations?user=4A1xYQwAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;|&nbsp; <a href="https://github.com/BradyFU">GitHub</a><br />  
Email: <img src="./images/email.png" alt="alt text" height="17"> <br />
</p>
</td></tr></table>
<hr />


<h2>Biography</h2>
<p>
å‚…æœå‹ï¼Œå—äº¬å¤§å­¦æ™ºèƒ½ç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢ç ”ç©¶å‘˜ã€åŠ©ç†æ•™æˆã€åšå¯¼ï¼Œå…¥é€‰ä¸­å›½ç§‘åâ€œé’å¹´äººæ‰æ‰˜ä¸¾å·¥ç¨‹â€ã€‚2022å¹´åšå£«æ¯•ä¸šäºä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€ã€‚ç ”ç©¶æ–¹å‘ä¸ºå¤šæ¨¡æ€æ™ºèƒ½ï¼Œè°·æ­Œå­¦æœ¯å¼•ç”¨6500ä½™æ¬¡ï¼Œä¸€ä½œå•ç¯‡å¼•ç”¨è¿‡åƒæ¬¡ï¼Œå…­ç¯‡ä¸€ä½œå•ç¯‡å¼•ç”¨è¿‡ç™¾æ¬¡ï¼Œå¼€æºé¡¹ç›®ç´¯è®¡è·å¾—2ä¸‡ä½™æ¬¡GitHub Starsã€‚ä»£è¡¨æ€§å·¥ä½œåŒ…æ‹¬VITAå¤šæ¨¡æ€å¤§æ¨¡å‹ç³»åˆ—ï¼ˆVITA-1.0/-1.5ã€Long-VITAã€VITA-Audioï¼‰ï¼ŒMMEå¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ç³»åˆ—ï¼ˆMMEã€Video-MMEã€MME-RealWorldï¼‰å’ŒAwesome-MLLMç¤¾åŒºç­‰ã€‚æ‹…ä»»Pattern RecognitionæœŸåˆŠç¼–å§”ã€ICLR/ICMLä¼šè®®é¢†åŸŸä¸»å¸­ã€CSIGé’å·¥å§”å§”å‘˜ã€CCF-AI/-CVä¸“å§”ä¼šæ‰§è¡Œå§”å‘˜ã€‚æ›¾è·å°ç±³é’å¹´å­¦è€…-ç§‘æŠ€åˆ›æ–°å¥–ã€åä¸ºç´«é‡‘å­¦è€…ã€ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šäº‘å¸†å¥–ã€ä¸­ç§‘é™¢é™¢é•¿ç‰¹åˆ«å¥–ã€IEEE Biometrics Council Best Doctoral Dissertation Awardã€åŒ—äº¬å¸‚ä¼˜åšã€ä¸­ç§‘é™¢ä¼˜åšã€CVPR 2023 Outstanding Reviewerã€‚
</p>
<br />
<p>
<span style="color: rgb(255, 0, 0)"> <strong> We are looking for self-motivated PhD and Master candidates! If you are interested, please feel free to contact me. Meanwhile, I am open to any discussion or collaboration.  </strong> </span> 
</p>
<br />
<p>
<img src="images/MiG_logo.jpg" alt="alt text" style="width: 85%; height: auto; display: block; margin: auto;"/>
</p>


<h2>Selected Publications</h2>
<table class="imgtable"><tr><td>
<img src="images/paper_vita-e.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2510.21817">VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting</a> <br />
Xiaoyu Liu, <strong>Chaoyou Fu [Corresponding Author]</strong>, Chi Yan, Chu Wu, et al <br />
arXiv 2025, <a href="https://lxysl.github.io/VITA-E">Project</a>
</p>
</td></tr></table>
    
<table class="imgtable"><tr><td>
<img src="images/paper_vita-audio-2.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2505.03739">VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model</a> <br />
Zuwei Long, Yunhang Shen, <strong>Chaoyou Fu [Corresponding Author]</strong>, Heting Gao, et al <br />
NeurIPS 2025, <a href="https://github.com/VITA-MLLM/VITA-Audio">Code</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_longvita.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2502.05177">Long-VITA: Scaling Large Multi-modal Models to 1 Million Tokens with Leading Short-Context Accuracy</a> <br />
Yunhang Shen, <strong>Chaoyou Fu [Corresponding Author]</strong>, Shaoqi Dong, Xiong Wang, Yi-Fan Zhang, et al <br />
arXiv 2025, <a href="https://github.com/VITA-MLLM/Long-VITA">Code</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_vita-1.5.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2501.01957">VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Haojia Lin, Xiong Wang, Yi-Fan Zhang, et al <br />
NeurIPS 2025 [Spotlight], <a href="https://github.com/VITA-MLLM/VITA">Code [2k+ Stars ğŸŒŸ]</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_vita.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2408.05211">VITA: Towards Open-Source Interactive Omni Multimodal LLM</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Haojia Lin, Zuwei Long, Yunhang Shen, et al <br />
arXiv 2024, <a href="https://vita-home.github.io/">Project</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_mme-survey.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2411.15296">MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Yi-Fan Zhang, Shukang Yin, Bo Li, et al <br />
arXiv 2024, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Benchmarks">Project</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_videomme.jpg" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2405.21075">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, et al <br />
CVPR 2025, <a href="https://video-mme.github.io/">Project</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_mme.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2306.13394.pdf">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a> <br />
<strong>Chaoyou Fu [Project Leader]</strong>, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, et al <br />
NeurIPS DB 2025 [Spotlight], <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation">Leaderboard [with 50+ MLLMs ğŸŒŸ]</a>, <a href="images/bib_mme.txt"><strong>Citation</strong></a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_mllm_v2.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2306.13549.pdf">A Survey on Multimodal Large Language Models</a> <br />
Shukang Yin, <strong>Chaoyou Fu [Project Leader]</strong>, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen <br />
National Science Review 2024, <a href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models">Project [10k+ Stars ğŸŒŸ]</a>, <a href="images/bib_survey.txt"><strong>Citation</strong></a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_woodpecker.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2310.16045.pdf">Woodpecker: Hallucination Correction for Multimodal Large Language Models</a> <br />
Shukang Yin, <strong>Chaoyou Fu [Project Leader]</strong>, Sirui Zhao, Tong Xu, et al <br />
arXiv 2023, <a href="https://github.com/BradyFU/Woodpecker">Code [500+ Stars ğŸŒŸ]</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_ape.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2312.02153.pdf">APE: Aligning and Prompting Everything All at Once for Universal Visual Perception</a> <br />
Yunhang Shen, <strong>Chaoyou Fu</strong>, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, et al <br />
CVPR 2024, <a href="https://github.com/shenyunhang/APE">Code [500+ Stars ğŸŒŸ]</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_dvg-face.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/2009.09399.pdf">DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition</a> <br />
<strong>Chaoyou Fu</strong>, Xiang Wu, Yibo Hu, Huaibo Huang, and Ran He <br />
TPAMI 2022, <a href="https://github.com/BradyFU/DVG-Face">Code</a> 
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_safe.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://ieeexplore.ieee.org/abstract/document/9971748">Towards Lightweight Pixel-Wise Hallucination for Heterogeneous Face Recognition</a> <br />
<strong>Chaoyou Fu</strong>, Xiaoqiang Zhou, Weizan He, and Ran He <br />
TPAMI 2023
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_mvf.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://arxiv.org/pdf/1903.12003.pdf">High Fidelity Face Manipulation with Extreme Poses and Expressions</a> <br />
<strong>Chaoyou Fu</strong>, Yibo Hu, Xiang Wu, Guoli Wang, Qian Zhang, and Ran He <br />
TIFS 2021, <a href="./projects/mvf-hq/mvf-hq.html">Dataset</a>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<img src="images/paper_cm-nas.png" alt="alt text" width="120" height="75"/> &nbsp;</td>
<td align="left">
<p> <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Fu_CM-NAS_Cross-Modality_Neural_Architecture_Search_for_Visible-Infrared_Person_Re-Identification_ICCV_2021_paper.pdf">CM-NAS: Cross-Modality Neural Architecture Search for Visible-Infrared Person Re-Identification</a> <br />
<strong>Chaoyou Fu</strong>, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei, and Ran He <br />
ICCV 2021, <a href="https://github.com/JDAI-CV/CM-NAS">Code</a>
</p>
</td></tr></table>



<h2>Academic Services</h2> 
<ul>
<li><p> Area Chair: ICLR, ICML 
<li><p> Associate Editor: Pattern Recognition
<li><p> Conference Reviewer: NeurIPS, ICLR, CVPR, ICCV, ECCV, AAAI, ACM MM, IJCAI </p></li>
<li><p> Journal Reviewer: IEEE TPAMI, IJCV, IEEE TIP </p></li>
</ul>




<h2>Honors and Awards</h2> 
<ul>
<li><p> [2025.07] ä¸–ç•Œäººå·¥æ™ºèƒ½å¤§ä¼šï¼ˆWAICï¼‰äº‘å¸†å¥–Â·æ˜æ—¥ä¹‹æ˜Ÿ </p></li>
<li><p> [2025.04] å—äº¬å¤§å­¦ç´«é‡‘å­¦è€… </p></li>
<li><p> [2025.03] <span style="color: #4f69c6"><strong>ç¬¬åå±Šä¸­å›½ç§‘åé’å¹´äººæ‰æ‰˜ä¸¾å·¥ç¨‹</strong></span> </p></li>
<li><p> [2024.11] å°ç±³é’å¹´å­¦è€…-ç§‘æŠ€åˆ›æ–°å¥– </p></li>
<li><p> [2023.12] åŒ—äº¬å¸‚ä¼˜ç§€åšå£«å­¦ä½è®ºæ–‡ </p></li>
<li><p> [2023.08] ä¸­å›½ç§‘å­¦é™¢ä¼˜ç§€åšå£«å­¦ä½è®ºæ–‡ </p></li>
<li><p> [2023.07] <span style="color: #4f69c6"><strong>IEEE Biometrics Council Best Doctoral Dissertation Award</strong></span> </p></li>
<li><p> [2023.07] CVPR 2023 Outstanding Reviewer (232/7000+) </p></li>
<li><p> [2022.07] <span style="color: #4f69c6"><strong>ä¸­å›½ç§‘å­¦é™¢é™¢é•¿ç‰¹åˆ«å¥–</strong></span> </p></li>
<li><p> [2022.07] åŒ—äº¬å¸‚ä¼˜ç§€æ¯•ä¸šç”Ÿ </p></li>
<li><p> [2021.12] 2022å¹´â€œ<span style="color: #4f69c6"><strong>è…¾è®¯æŠ€æœ¯å¤§å’–</strong></span>â€è®¡åˆ’-T10 </p></li>
<li><p> [2021.12] 2022å¹´â€œ<span style="color: #4f69c6"><strong>é˜¿é‡Œæ˜Ÿ</strong></span>â€è®¡åˆ’-P7 </p></li>
<li><p> [2021.12] åšå£«ç ”ç©¶ç”Ÿå›½å®¶å¥–å­¦é‡‘ </p></li>
<li><p> [2021.11] å®é’¢å¥–å­¦é‡‘ä¼˜ç§€å­¦ç”Ÿå¥– </p></li>
<li><p> [2019.12] ç¡•å£«ç ”ç©¶ç”Ÿå›½å®¶å¥–å­¦é‡‘ </p></li>
<li><p> [2017.06] å®‰å¾½çœä¼˜ç§€æ¯•ä¸šç”Ÿ </p></li>
<li><p> [2015.11] æœ¬ç§‘ç”Ÿå›½å®¶å¥–å­¦é‡‘ </p></li>
<li><p> [2015.08] â€œé£æ€å¡å°”â€æ¯å…¨å›½å¤§å­¦ç”Ÿæ™ºèƒ½æ±½è½¦ç«èµ›å…¨å›½æ€»å†³èµ›äºŒç­‰å¥– </p></li>
</ul>


</body>
</html>
